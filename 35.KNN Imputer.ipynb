{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388f8583",
   "metadata": {},
   "source": [
    "| Age | Fare | SibSp |\n",
    "|----|----|----|\n",
    "| 22 | 7.25 | 1 |\n",
    "| NaN | 7.30 | 1 |\n",
    "| 24 | NaN | 0 |\n",
    "| 23 | 7.20 | 1 |\n",
    "\n",
    "- __Mujhe second row me 22 k bad wala fill krna h to use fill krne k lie euclidien distance nikalenge or fir dekhnge kon c row usk pass h jo row us k pass hui uska data use kr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc7d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "580ced8b",
   "metadata": {},
   "source": [
    "## ðŸ“Œ KNN Multivariate Imputation\n",
    "\n",
    "### âœ… Proper Definition\n",
    "**KNN multivariate imputation** is a missing-data handling technique in which the missing value of a feature is estimated using the values from the **K most similar observations**, where similarity is computed using **multiple features simultaneously**.\n",
    "\n",
    "> Unlike univariate methods (mean/median), KNN imputation uses **relationships across several columns and rows** to fill missing values.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Key Concept (Why â€œMultivariateâ€?)\n",
    "- Distance between rows is calculated using **multiple variables**\n",
    "- Each row is treated as a **point in multi-dimensional space**\n",
    "- Nearest rows are found based on all available features\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ Mathematical Intuition\n",
    "\n",
    "Given a dataset  \n",
    "$$\n",
    "X = \\{x_1, x_2, \\dots, x_n\\}\n",
    "$$\n",
    "\n",
    "For a row \\( x_i \\) with missing value in feature \\( j \\):\n",
    "\n",
    "1. Compute distance between \\( x_i \\) and all other rows using **available features**\n",
    "2. Select **K nearest rows**\n",
    "3. Impute the missing value as:\n",
    "   - **Mean** of neighbors (numeric)\n",
    "   - **Mode** of neighbors (categorical after encoding)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§ª Example (Multi-Row, Multi-Column)\n",
    "\n",
    "#### Original Data\n",
    "\n",
    "| Age | Fare | SibSp |\n",
    "|----|----|----|\n",
    "| 22 | 7.25 | 1 |\n",
    "| NaN | 7.30 | 1 |\n",
    "| 24 | NaN | 0 |\n",
    "| 23 | 7.20 | 1 |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Imputation\n",
    "\n",
    "**Step 1:** Find similar rows using *Age, Fare, SibSp*  \n",
    "**Step 2:** Select `k = 2` nearest rows  \n",
    "**Step 3:** Fill missing values using neighbors\n",
    "\n",
    "---\n",
    "\n",
    "#### After KNN Multivariate Imputation\n",
    "\n",
    "| Age | Fare | SibSp |\n",
    "|----|----|----|\n",
    "| 22 | 7.25 | 1 |\n",
    "| 23 | 7.30 | 1 |\n",
    "| 24 | 7.23 | 0 |\n",
    "| 23 | 7.20 | 1 |\n",
    "\n",
    "âœ” Age and Fare are filled using **multiple rows & features**\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ How KNN Imputer is Applied (sklearn)\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling is important\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# KNN Imputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "X_imputed = imputer.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404f8ee",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Missing Value Imputation & KNN â€” Complete Notes with Formulas\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Missing Values\n",
    "\n",
    "### ðŸ“Œ Definition\n",
    "Missing values are observations where data is not recorded.\n",
    "They are commonly represented as `NaN` or `None`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Missing Indicator\n",
    "\n",
    "### âœ… Definition\n",
    "A **missing indicator** is a binary variable that indicates whether the original value of a feature was missing.\n",
    "\n",
    "### ðŸ“ Formula\n",
    "For a feature \\( X \\):\n",
    "\n",
    "$$\n",
    "M =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } X \\text{ is missing} \\\\\n",
    "0, & \\text{if } X \\text{ is present}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Simple Imputation (Automatic)\n",
    "\n",
    "### ðŸ“Œ Definition\n",
    "Simple imputation replaces missing values using a statistic computed from the observed data.\n",
    "\n",
    "### ðŸ“ Formulas\n",
    "\n",
    "#### Mean Imputation\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "#### Median Imputation\n",
    "$$\n",
    "\\text{Median}(X)\n",
    "$$\n",
    "\n",
    "#### Mode Imputation\n",
    "$$\n",
    "\\text{Mode}(X)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Random Value Imputation\n",
    "\n",
    "### ðŸ“Œ Definition\n",
    "Random value imputation replaces a missing value by randomly sampling from observed values of the same feature.\n",
    "\n",
    "### ðŸ“ Formula\n",
    "$$\n",
    "x_{\\text{miss}} \\sim \\text{RandomSample}(X_{\\text{observed}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ KNN (K-Nearest Neighbors)\n",
    "\n",
    "### ðŸ“Œ Definition\n",
    "KNN is an instance-based algorithm that predicts values based on the **K closest observations**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Distance Measures in KNN\n",
    "\n",
    "### ðŸ”¹ Euclidean Distance\n",
    "$$\n",
    "d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "### ðŸ”¹ Manhattan Distance\n",
    "$$\n",
    "d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|\n",
    "$$\n",
    "\n",
    "### ðŸ”¹ Minkowski Distance\n",
    "$$\n",
    "d(x, y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^p\\right)^{1/p}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ KNN Classification\n",
    "\n",
    "### ðŸ“ Majority Voting Formula\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{c} \\sum_{x_i \\in N_k(x)} I(y_i = c)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(N_k(x)\\): K nearest neighbors  \n",
    "- \\(I(\\cdot)\\): Indicator function  \n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ KNN Regression\n",
    "\n",
    "### ðŸ“ Mean-Based Prediction\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{K} \\sum_{x_i \\in N_k(x)} y_i\n",
    "$$\n",
    "\n",
    "### ðŸ“ Distance-Weighted Prediction\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\frac{\\sum_{x_i \\in N_k(x)} \\frac{1}{d(x,x_i)} y_i}\n",
    "{\\sum_{x_i \\in N_k(x)} \\frac{1}{d(x,x_i)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Feature Scaling (Required for KNN)\n",
    "\n",
    "### ðŸ”¹ Standardization\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "### ðŸ”¹ Minâ€“Max Scaling\n",
    "$$\n",
    "x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ÿ KNN Multivariate Imputation\n",
    "\n",
    "### ðŸ“Œ Definition\n",
    "KNN multivariate imputation estimates missing values using **K nearest rows**, where distance is computed using **multiple features**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ Distance Computation\n",
    "$$\n",
    "d(x_i, x_j) = \\sqrt{\\sum_{m \\in M}(x_{im} - x_{jm})^2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(M\\) = set of non-missing features\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ Imputation Formula (Numeric)\n",
    "\n",
    "For missing value $(x_{ij}\\)$:\n",
    "\n",
    "$$\n",
    "\\hat{x}_{ij} = \\frac{1}{K} \\sum_{x_l \\in N_k(i)} x_{lj}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ Distance-Weighted Imputation\n",
    "$$\n",
    "\\hat{x}_{ij} =\n",
    "\\frac{\\sum_{x_l \\in N_k(i)} \\frac{1}{d(i,l)} x_{lj}}\n",
    "{\\sum_{x_l \\in N_k(i)} \\frac{1}{d(i,l)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 11ï¸âƒ£ sklearn Implementation\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "X_imputed = imputer.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df98268",
   "metadata": {},
   "source": [
    "## ðŸ“Œ What Does â€œSelecting the Nearest Neighborâ€ Mean?\n",
    "\n",
    "### âœ… Simple Meaning\n",
    "**Selecting the nearest neighbor** means **choosing the data points (rows) that are most similar to a given data point**, based on a distance measure.\n",
    "\n",
    "> â€œNearestâ€ = **smallest distance**  \n",
    "> â€œNeighborâ€ = **another data point (row) in the dataset**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Intuition\n",
    "Imagine each row of data as a **point in space**.\n",
    "- Points that are **closer together** are more similar\n",
    "- KNN selects the points that lie **closest** to the target point\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¢ How Similarity Is Measured\n",
    "Similarity is measured using **distance**.\n",
    "\n",
    "### Example (Euclidean Distance)\n",
    "$$\n",
    "d(x, y) = \\sqrt{\\sum (x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "Smaller distance â‡’ more similar â‡’ nearer neighbor\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Simple Example\n",
    "\n",
    "### Dataset (2 features)\n",
    "\n",
    "| Row | Age | Fare |\n",
    "|----|----|----|\n",
    "| A | 22 | 7.25 |\n",
    "| B | 23 | 7.30 |\n",
    "| C | 40 | 80.00 |\n",
    "\n",
    "Suppose we want neighbors for:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0443e",
   "metadata": {},
   "source": [
    "## ðŸ“Œ What is Non-Euclidean Distance?\n",
    "\n",
    "### âœ… Definition\n",
    "A **non-Euclidean distance** is any distance measure that **does not use the straight-line (Euclidean) formula** to calculate similarity between data points.\n",
    "\n",
    "> **In simple words:**  \n",
    "> If distance is **not calculated using the Pythagorean (straight-line) formula**, it is called **non-Euclidean distance**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Why Non-Euclidean Distances Exist\n",
    "Euclidean distance assumes:\n",
    "- Continuous numeric data  \n",
    "- Same scale  \n",
    "- Straight-line geometry  \n",
    "\n",
    "But real-world data can be:\n",
    "- Categorical  \n",
    "- Binary  \n",
    "- Sparse  \n",
    "- Text-based  \n",
    "- Network / graph-based  \n",
    "\n",
    "ðŸ‘‰ For such data, Euclidean distance is **not suitable**, so we use **non-Euclidean distances**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¢ Common Non-Euclidean Distance Measures\n",
    "\n",
    "### ðŸ”¹ 1. Manhattan Distance (L1 Distance)\n",
    "Measures distance as the **sum of absolute differences**.\n",
    "\n",
    "$$\n",
    "d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|\n",
    "$$\n",
    "\n",
    "ðŸ“Œ Used when:\n",
    "- Grid-like paths (city blocks)\n",
    "- More robust to outliers than Euclidean distance\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 2. Chebyshev Distance\n",
    "Measures the **maximum absolute difference** among all dimensions.\n",
    "\n",
    "$$\n",
    "d(x, y) = \\max \\left( |x_i - y_i| \\right)\n",
    "$$\n",
    "\n",
    "ðŸ“Œ Used in:\n",
    "- Chess (kingâ€™s movement)\n",
    "- Worst-case distance measurement\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 3. Minkowski Distance (Generalized Distance)\n",
    "A generalized form that includes multiple distance metrics.\n",
    "\n",
    "$$\n",
    "d(x, y) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p}\n",
    "$$\n",
    "\n",
    "- \\( p = 1 \\) â†’ Manhattan distance  \n",
    "- \\( p = 2 \\) â†’ Euclidean distance  \n",
    "- \\( p \\neq 2 \\) â†’ **Non-Euclidean distance**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 4. Hamming Distance\n",
    "Counts the **number of positions where values differ**.\n",
    "\n",
    "$$\n",
    "d(x, y) = \\sum I(x_i \\neq y_i)\n",
    "$$\n",
    "\n",
    "ðŸ“Œ Used for:\n",
    "- Binary data  \n",
    "- Categorical variables  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 5. Cosine Distance\n",
    "Measures the **angle between vectors**, not their magnitude.\n",
    "\n",
    "$$\n",
    "\\text{Cosine Distance} = 1 - \\frac{x \\cdot y}{\\|x\\| \\, \\|y\\|}\n",
    "$$\n",
    "\n",
    "ðŸ“Œ Used in:\n",
    "- Text mining  \n",
    "- NLP  \n",
    "- Recommendation systems  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 6. Jaccard Distance\n",
    "Measures **dissimilarity between two sets**.\n",
    "\n",
    "$$\n",
    "d(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "\n",
    "ðŸ“Œ Used for:\n",
    "- Binary features  \n",
    "- Set similarity analysis  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ Euclidean vs Non-Euclidean (Quick Comparison)\n",
    "\n",
    "| Aspect | Euclidean | Non-Euclidean |\n",
    "|------|----------|---------------|\n",
    "| Geometry | Straight line | Alternative geometry |\n",
    "| Data type | Numeric only | Numeric, categorical, text |\n",
    "| Scaling sensitivity | High | Metric-dependent |\n",
    "| Flexibility | Low | High |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  In KNN Context\n",
    "- KNN **does not require Euclidean distance**\n",
    "- Any valid distance metric can be used\n",
    "- Choice of distance depends on the **nature of data**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  One-Line Intuition\n",
    "> **Non-Euclidean distance measures similarity without using straight-line geometry, making it suitable for complex or non-numeric data.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fc48d",
   "metadata": {},
   "source": [
    "# ðŸ“˜ KNN Imputer â€“ Working (Day 39)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Given Data Table\n",
    "\n",
    "| Index | Feature 1 | Feature 2 | Feature 3 | Target |\n",
    "|------:|----------:|----------:|----------:|-------:|\n",
    "| 1 | 33 | 67 | 21 | --- |\n",
    "| 2 | **31.5** | 68 | 12 | --- |\n",
    "| 3 | 23 | 71 | 18 | --- |\n",
    "| 4 | 40 | 81 | --- | --- |\n",
    "| 5 | 35 | 79 | --- | --- |\n",
    "\n",
    "> ðŸ”¹ **31.5** â†’ Imputed value  \n",
    "> ðŸ”¹ KNN is used to compute distances from other points\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Distance Calculations (Euclidean Distance)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Distance from Point 1 (pt1)\n",
    "\n",
    "$$\n",
    "d = \\sqrt{\\frac{3}{2}\\left((68 - 67)^2 + (12 - 21)^2\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{1.5 \\times (1 + 81)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{127.5} = \\mathbf{11.29}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Distance from Point 3 (pt3)\n",
    "\n",
    "$$\n",
    "d = \\sqrt{\\frac{3}{3}\\left((51 - 45)^2 + (71 - 68)^2 + (18 - 12)^2\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{36 + 9 + 36}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{81} = \\mathbf{9}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Distance from Point 4 (pt4)\n",
    "\n",
    "$$\n",
    "d = \\sqrt{3 \\times (81 - 68)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{3 \\times 9}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{27} = \\mathbf{5.19}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Distance from Point 5 (pt5)\n",
    "\n",
    "$$\n",
    "d = \\sqrt{\\frac{3}{2}\\left((60 - 45)^2 + (79 - 68)^2\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{1.5 \\times (25 + 121)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{219} = \\mathbf{14.79}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary of Distances\n",
    "\n",
    "| Point | Distance |\n",
    "|------:|---------:|\n",
    "| pt1 | 11.29 |\n",
    "| pt3 | 9.00 |\n",
    "| pt4 | **5.19 (Nearest)** |\n",
    "| pt5 | 14.79 |\n",
    "\n",
    "---\n",
    "\n",
    "## âš–ï¸ Advantages & Disadvantages\n",
    "\n",
    "### âœ… Advantages\n",
    "- Uses similarity between data points\n",
    "- Works well for numerical features\n",
    "\n",
    "### âŒ Disadvantages\n",
    "- Computationally expensive\n",
    "- Sensitive to feature scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c0677c",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Why the Imputed Value is **31.5**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” What is Missing?\n",
    "- The missing value is in **Feature-1** for one data point.\n",
    "- We estimate this missing value using **K-Nearest Neighbors (KNN)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Step 1: Find the Nearest Points\n",
    "\n",
    "From the distance calculations:\n",
    "\n",
    "| Point | Distance |\n",
    "|------:|---------:|\n",
    "| pt1 | 11.29 |\n",
    "| pt3 | 9.00 |\n",
    "| pt4 | **5.19** |\n",
    "| pt5 | 14.79 |\n",
    "\n",
    "âž¡ The smallest distance is **pt4**, so it is the closest neighbor.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Step 2: Choose the Value of K\n",
    "\n",
    "- If **K = 1**  \n",
    "  â†’ Missing value = Feature-1 of pt4 = **40**\n",
    "\n",
    "- But the imputed value is **31.5**, so **K = 2** is used.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Step 3: Identify the Two Nearest Neighbors\n",
    "\n",
    "| Point | Distance | Feature-1 |\n",
    "|------:|---------:|----------:|\n",
    "| pt4 | 5.19 | 40 |\n",
    "| pt3 | 9.00 | 23 |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® Step 4: Compute the Mean\n",
    "\n",
    "$$\n",
    "\\text{Imputed Value}\n",
    "= \\frac{40 + 23}{2}\n",
    "= \\frac{63}{2}\n",
    "= \\mathbf{31.5}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Final Conclusion\n",
    "\n",
    "> The missing value is obtained by averaging the Feature-1 values of the **two nearest neighbors (K = 2)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Exam-Oriented One-Liner\n",
    "> The missing value is calculated by taking the mean of the K nearest neighbors.  \n",
    "> Here, K = 2 and the values are 40 and 23, so the imputed value is **31.5**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eeaaf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import SimpleImputer,KNNImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad1c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\Lenovo\\Krishnaraj singh\\Code\\newml\\Documents!.0\\train.csv',usecols=['Age','Pclass','Fare','Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77531498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>9.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>39.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.8292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.8542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.2583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass   Age     Fare\n",
       "873         0       3  47.0   9.0000\n",
       "556         1       1  48.0  39.6000\n",
       "653         1       3   NaN   7.8292\n",
       "623         0       3  21.0   7.8542\n",
       "864         0       2  24.0  13.0000\n",
       "46          0       3   NaN  15.5000\n",
       "858         1       3  24.0  19.2583"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a49538b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Age         177\n",
       "Fare          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16fe1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=['Survived'])\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58e4a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f27c3b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12.650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass   Age    Fare\n",
       "787       3   8.0  29.125\n",
       "399       2  28.0  12.650"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f30f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNImputer(n_neighbors=5,weights='distance')\n",
    "\n",
    "x_train_trf = knn.fit_transform(x_train)\n",
    "x_test_trf = knn.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acd2c125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7039106145251397"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(x_train_trf,y_train)\n",
    "\n",
    "y_predict = lr.predict(x_test_trf)\n",
    "\n",
    "accuracy_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85248aef",
   "metadata": {},
   "source": [
    "__Compare with simple imputer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74bbae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "si = SimpleImputer()\n",
    "\n",
    "x_train_trf1 = si.fit_transform(x_train)\n",
    "x_test_trf1 = si.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9b02c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6927374301675978"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train_trf1,y_train)\n",
    "\n",
    "y_predict1 = lr.predict(x_test_trf1)\n",
    "\n",
    "accuracy_score(y_test,y_predict1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
